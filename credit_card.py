# -*- coding: utf-8 -*-
"""credit_card.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VqogR9Lw7b5Tqeh_ReXFfa801JgRUEMK

## **Importing Libraries**
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

# connecting to the google drive
from google.colab import drive
drive.mount('/content/drive')

# Displaying the all the rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

"""## **Loading the dataset**"""

df=pd.read_csv('/content/drive/MyDrive/creditcard.csv')
df.sample(5)

df.shape

df.isnull().sum()

df.info()

"""## **Distribution of transactions **"""

df['Class'].value_counts()

"""The dataset is highly unbalanced"""

# separating the data

legal=df[df["Class"]==0]
fake=df[df.Class==1]

print(legal.shape)
print(fake.shape)

legal.Amount.describe()

fake.Amount.describe()

"""Transforming Amount column into same scale of differnt columns"""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
df.Amount=sc.fit_transform(pd.DataFrame(df.Amount))

df.head()

"""## Feature Selection"""

data=df.drop(['Time'],axis=1)

# checking for duplicates
data.duplicated().sum()

# removing the duplicates
data=data.drop_duplicates()
data.shape

"""Checking wheather the dataset is balanced or not"""

data.Class.value_counts()

# training the dataset with imbalnced data
X=data.drop(['Class'],axis=1)
y=data.Class

X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=42)

print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)

classifier={
        "Logistic Regression":LogisticRegression(),
    "Decision Tree":DecisionTreeClassifier(),
    "Random Forest":RandomForestClassifier(),
    # "KNN":KNeighborsClassifier()
}


for name, classifier in classifier.items():
  print(name)
  classifier.fit(X_train,Y_train)
  train_pred=classifier.predict(X_test)
  print("Accuracy: ",accuracy_score(Y_test,train_pred))
  print("precision ",precision_score(Y_test,train_pred))
  print("recall   ",recall_score(Y_test,train_pred))
  print("F1 score ",f1_score(Y_test,train_pred))



"""As data is unbalanced we can perform either Under-sampling or Over sampling

**Under-Sampling**: building a dataset which contains same number of transactions of both fake (or) fradulent and legit (or) legal transactions
"""

# undersampling
normal=data[data['Class']==0]
fake=data[data['Class']==1]

fake.shape

# creating the new dataset

legal_s=data.sample(473)
legal_s.shape

balanced_df=pd.concat([legal_s,fake],axis=0)
balanced_df.value_counts('Class')

X=balanced_df.drop(['Class'],axis=1)
y=balanced_df.Class

X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=42)

classifier={
        "Logistic Regression":LogisticRegression(),
    "Decision Tree":DecisionTreeClassifier(),
    "Random Forest":RandomForestClassifier(),
    # "KNN":KNeighborsClassifier()
}


for name, classifier in classifier.items():
  print(name)
  classifier.fit(X_train,Y_train)
  train_pred=classifier.predict(X_test)
  print("Accuracy: ",accuracy_score(Y_test,train_pred))
  print("precision ",precision_score(Y_test,train_pred))
  print("recall   ",recall_score(Y_test,train_pred))
  print("F1 score ",f1_score(Y_test,train_pred))

# oversampling

X=data.drop(['Class'],axis=1)
y=data.Class

y.shape

from imblearn.over_sampling import SMOTE

X_res,y_res =SMOTE().fit_resample(X,y)

X_res.shape

y_res.value_counts()

X_train,X_test,Y_train,Y_test=train_test_split(X_res,y_res,test_size=0.2,random_state=42)

classifier={
        "Logistic Regression":LogisticRegression(),
    "Decision Tree":DecisionTreeClassifier(),
    "Random Forest":RandomForestClassifier(),
    "KNN":KNeighborsClassifier()
}


for name, classifier in classifier.items():
  print(name)
  classifier.fit(X_train,Y_train)
  train_pred=classifier.predict(X_test)
  print("Accuracy: ",accuracy_score(Y_test,train_pred))
  print("precision ",precision_score(Y_test,train_pred))
  print("recall   ",recall_score(Y_test,train_pred))
  print("F1 score ",f1_score(Y_test,train_pred))

"""# **Random Forest is more accurate, handles complex data better, and is more reliable than both Logistic Regression and Decision Tree.**







"""

model=RandomForestClassifier()
model.fit(X_train,Y_train)

"""Predicting the test data

"""

print("accuracy of test data by using Random forest: ",accuracy_score(model.predict(X_test),Y_test))